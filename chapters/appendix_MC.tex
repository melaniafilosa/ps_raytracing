\chapter{Monte Carlo integration versus Monte Carlo ray tracing}\label{chap:appraytracing}
Although Monte Carlo (MC)	 integration and MC ray tracing are two different methods, they are based on the same idea. To fully comprehend MC ray tracing it is necessary to first understand the basics of MC integration. The aim of this appendix is to point out the similarities and the differences between the two methods. In the next section we give an introduction to MC integration for the two-dimensional case. In Section \ref{app:MCRT} we explain MC ray tracing.
\section{Monte Carlo integration}
Let us consider a set $\mbox{\insieme{D}} = [\vect{a}, \vect{b}]= [a_1, b_1] \times [a_2, b_2]$ with $\vect{a} = (a_1, a_2)$ and $\vect{b} = (b_1, b_2)\in\mathbb{R}^2$. Consider a function $f:[\vect{a},\vect{b}]\subset \mathbb{R}^2 \rightarrow \mathbb{R}$ and a random variable $\vect{Y}$ with values in \insieme{D} and probability density function $\rho(\vect{y})$, where $\vect{y}$ are the values of $\vect{Y}$ in \insieme{D}. We indicate the random variables with capital letters and the corresponding deterministic values with lowercase letters. The expected value of $f(\vect{Y})$ with respect of $\rho(\vect{Y})$ is:
\begin{equation}
\mathbb{E}[f(\vect{Y})] =\int_{\textup{D}}f(\vect{y})\rho(\vect{y}) \textrm{d}\vect{y}.
\end{equation}
If $\rho$ is a uniform probability density function, the expected value is given by:
\begin{equation}\label{eq:expected_value}
\mathbb{E}[f(\vect{Y})] =\frac{1}{\lambda([\vect{a},\vect{b}])} \int_{\textup{D}}f(\vect{y}) \textrm{d}\vect{y},
\end{equation}
where $\lambda([\vect{a},\vect{b}]) = (b_1-a_1)\cdot(b_2-a_2)$.
%In this case, MC methods approximate the integral in Equation (\ref{eq:expected_value}) by the sum:
Let $\{\vect{Y}_\variabile{i}\}_{\variabile{i} = 1, \cdots, N}$ be independent samples with probability density function $\rho$.
Indicating with $S_{N}[f(\vect{Y})]$ the average: 
\begin{equation}
S_{N}[f(\vect{Y})] = \frac{1}{N} \sum_{\variabile{i}=1}^{N} f(\vect{Y}_\variabile{i}),
\end{equation}
and with $\const{Pr}(E)$ the probability that an event $E$ happens,
%\begin{equation}\label{eq:approx_MC_QMC}
%\frac{1}{\lambda([\vect{a},\vect{b}])}\int_{\textup{D}}f(\vect{y}) \textrm{d}\vect{y}\approx\frac{1}{N} \sum_{\variabile{i}=1}^{N} f(\vect{Y}_\variabile{i}).
%\end{equation}
the strong law of large numbers states that
\begin{equation}
\const{Pr}\Big(\lim_{N\rightarrow\infty}S_{N}[f(\vect{Y})] = \mathbb{E}[f(\vect{Y})] \Big) = 1
\end{equation}
\cite{grinstead2012introduction}. This implies that the empirical mean is an unbiased estimator of the expectation value:
\begin{equation}\label{eq:approx_MC_QMC}
\mathbb{E}[f(\vect{Y})] \approx \frac{1}{N} \sum_{\variabile{i}=1}^{N} f(\vect{Y}_\variabile{i}).
\end{equation}
From the linearity of the expected value (see \cite{grinstead2012introduction})
%\footnote{Given a set of independent random variables $\{X_\variabile{i}\}_{\variabile{i}=1, \cdots, N}$ and a real number $a$, the expected value satisfies:
%$\mathbb{E}\big[\sum_{\variabile{i}=1}^{N}X_{\variabile{i}}\big] = \sum_{\variabile{i}=1}^{N}\mathbb{E}[X_{\variabile{i}}]$ and $\mathbb{E}[a\,X_\variabile{i}] = a\mathbb{E}[X_{\variabile{i}}]$.},
 it follows that
\begin{equation}\label{eq:linearity}
\mathbb{E}[S_{N}[f(\vect{Y})]] = \frac{1}{N}\sum_{\variabile{i}=1}^{\textrm{N}}\mathbb{E}[f(\vect{Y}_{\variabile{i}})] = \mathbb{E}[f(\vect{Y})],
\end{equation}
where the second equality holds as
$f(\vect{Y}_{\variabile{i}})_{\variabile{i}=1, \cdots, N}$ are random variables with probability density function $\rho(\vect{y})$, because $\big(\vect{Y}_{\variabile{i}}\big)_{\variabile{i}=1, \cdots, N}$ are independent samples with probability density function $\rho(\vect{y})$. This implies that $\mathbb{E}[f(\vect{Y}_{\variabile{i}})] = \mathbb{E}[f(\vect{Y})]$ for every $\variabile{i}=1, \cdots, N$. Indicating with $\textrm{Var}[f(\vect{Y})]$ the variance of $f(\vect{Y})$:
\begin{equation}\textrm{Var}[f(\vect{Y})]=\mathbb{E}[(f(\vect{Y})-\mathbb{E}[f(\vect{Y})])^2] = \sigma^2[f(\vect{Y})],
\end{equation}
the Bienaym\'e formula (see \cite{grinstead2012introduction}) can be applied because the random variables $\{\vect{Y}_\variabile{i}\}_{i = 1, \cdots, N}$ are uncorrelated, therefore
%\footnote{Given a set of \textit{independent} random variables $\{X_\variabile{i}\}_{\variabile{i}=1, \cdots, N}$ and a real number $a$, the variance satisfies:
%$\textrm{Var}\big[\sum_{\variabile{i}=1}^{N}X_{\variabile{i}}\big] = \sum_{\variabile{i}=1}^{N}\textrm[X_{\variabile{i}}]$ and $\textrm{Var}[a\,X_\variabile{i}] = a^2\textrm{Var}[X_{\variabile{i}}]$.} 
\begin{equation}\label{eq:variance}
\textrm{Var}[S_{N}[f(\vect{Y})]]= \textrm{Var}\Bigg[\frac{1}{N}\sum_{\variabile{i}=1}^{N}f(\vect{Y}_\variabile{i})\Bigg] =
 \frac{1}{N^2}\sum_{\variabile{i}=1}^{N}\textrm{Var}\big[f(\vect{Y}_\variabile{i})\big] = \frac{1}{N}\textrm{Var}[f(\vect{Y})], 
\end{equation} 
where the last equality holds as $\textrm{Var}[f(\vect{Y}_{\variabile{i}})] = \textrm{Var}[f(\vect{Y})]$.
Equation (\ref{eq:linearity}) gives
\begin{equation}\label{eq:variance2}
\begin{aligned}
\textrm{Var}[S_N[f(\vect{Y})]] &= \mathbb{E}[(S_{N}[f(\vect{Y})]-\mathbb{E}[S_{N}[f(\vect{Y})]])^2]\\ &= \mathbb{E}[(S_{N}[f(\vect{Y})]-\mathbb{E}[f(\vect{Y})])^2].
\end{aligned}
\end{equation}
Combining (\ref{eq:variance}) with (\ref{eq:variance2}) we obtain:
\begin{equation}\label{eq:variance3}
\mathbb{E}[(S_{N}[f(\vect{Y})]-\mathbb{E}[f(\vect{Y})])^2] = \sigma^2[f(\vect{Y})]/N.
\end{equation}
Let us denote the integration error with:
\begin{equation}\label{eq:error1}
\const{err}(f, S_{N}[f(\vect{Y})]) =\int_{\textup{D}}f(\vect{y})\rho(\vect{y}) \textrm{d}\vect{y}-S_{N}[f] = \mathbb{E}[f(\vect{Y})]-S_N[f(\vect{Y})].
\end{equation}
Considering the convex function $g:\variabile{x}\mapsto\variabile{x}^2$ we can write:
\begin{equation}
\begin{aligned}
\mathbb{E}[|\const{err}(f, S_{N})|] &= \sqrt{g\big(\mathbb{E}[|\const{err}(f, S_{N})|]\big)} \\&\leq\sqrt{\mathbb{E}[g(|\const{err}(f, S_{N})|)]} = \sqrt{\mathbb{E}[\const{err}^2(f, S_{N})]},
\end{aligned}
%\begin{aligned}
%\mathbb{E}\big[|\const{err}(f, S_{\const{N}})|\big] &= \frac{1}{\const{N}}\sqrt{\Bigg(\sum_{\variabile{1}=1}^\const{N}|\const{err}(f, S_{\const{N}})|\Bigg)^2}  \leq \frac{1}{\const{N}}\sqrt{\const{N}\sum_{\variabile{1}=1}^\const{N}\big(\const{err}(f, S_{\const{N}})\big)^2}  \\
%& =\sqrt{\frac{1}{\const{N}}\sum_{\variabile{1}=1}^\const{N}\big(\const{err}(f, S_{\const{N}})\big)^2}= \sqrt{\mathbb{E}\big[\const{err}(f, S_{\const{N}})^2\big]}
%\end{aligned}
\end{equation} 
where the inequality follows from Jensen's inequality (\cite{williams1991probability} Chapter $6$).
Using the previous relation and Equations (\ref{eq:variance3}) and (\ref{eq:error1}), we obtain:
\begin{equation}\label{eq:mean_error}
\mathbb{E}\big[|\const{err}(f, S_{N})|\big]\leq
\sqrt{\mathbb{E}\big[\const{err}^2(f, S_{N})\big]} = \frac{\sigma[f]}{\sqrt{N}}.
\end{equation}
Hence, the absolute value of the integration error is, on average, bounded by $\sigma[f(\vect{Y})]/\sqrt{N}$, where $\sigma[f(\vect{Y})]$ is the standard deviation of $f$ \cite{leobacher2014introduction}. It is important to note that $\const{err}(f, S_{N})$ does not depend on the dimension of the set \insieme{D}.
\section{Monte Carlo ray tracing}\label{app:MCRT}
The MC technique can be combined with ray tracing in order to compute the intensity distribution at the target of an optical system.
In MC ray tracing the position and the direction of every ray are chosen randomly at the source. 

In the two-dimensional case, for every ray we need to choose one position coordinate $\variabile{x}_1$ and one angular coordinate $\optangle_1$. These coordinates are chosen randomly and such that they are uniformly distributed at the source. 
Using ray tracing, the rays with those random initial coordinates are traced from \point{S} to \point{T} and, the corresponding random variables $\vect{Z}_{\variabile{i}}=(\variabile{x}^{\variabile{i}},\optangle^{\variabile{i}})$ at the target are obtained, where $\variabile{x}^{\variabile{i}}$ and $\variabile{t}^{\variabile{i}}$ are the position and direction coordinates of the $\variabile{i}$-th ray at the target. Note that the rays at the target are most likely non-uniformly distributed. Once the target variables are computed, we can apply the idea of MC simulation to approximate the target intensity $I(\optangle)$ defined in Chapter \ref{chap:raytracing}. Since the intensity only depends on the angular coordinate $\optangle$ we use MC approximation in one dimension. Hence, we approximate the expected value $\mathbb{E}[I]$ by a sum as described in Equation (\ref{eq:approx_MC_QMC}). 

To clarify the connection between MC integration and MC ray tracing, in Table \ref{tab:MCcomparison} we report the correspondence between the functions and the variables used in the previous section and in Chapter \ref{chap:raytracing}.
\begin{table}[h] \label{tab:MCcomparison}
\centering
\caption{\bf Comparison between MC integration and MC ray tracing}
\begin{tabular}{|l|l|l|}
 \hline   MC integration & MC ray tracing \\
  \hline 
 \insieme{D} $= [\vect{a}, \vect{b}]\subset \mathbb{R}^2$  & $[\optangle_{\variabile{j}-1}, \optangle_{\variabile{j}})\subset \mathbb{R}$ \\ [0.9ex] 
 $f:\mbox{\insieme{D}}\rightarrow \mathbb{R}^2$ & $\chi_{\variabile{j}}: [\optangle_{\variabile{j}-1}, \optangle_{\variabile{j}})\rightarrow \{0,1\}$ \\ [0.9ex] 
 $\vect{Y}$   & $\optangle$ \\ [0.9ex] 
 $N$   & $\nrays$ \\ [0.9ex] 
$\{\vect{Y}_{\variabile{j}}\}_{\variabile{j}=1, \cdots, N}$  
& $\{\optangle^{\variabile{k}}\}_{\variabile{k}=1, \cdots, \nrays}$  \\ [0.9ex] 
 $S_{N}[f(\vect{Y})] = \sum_{\variabile{i}=1}^N \frac{f(\vect{Y}_{\variabile{i}})}{N}$ &
 $\hat{I}_{\textrm{MC}}  = \frac{\nrays[\optangle_{\variabile{j}-1}, \optangle_{\variabile{j}})}{\nrays[-\pi/2, \pi/2]}= \frac{1}{\nrays}\sum_{\variabile{k}=1}^{\nrays}\chi_{\variabile{j}}(\optangle^{\variabile{k}})$ \\ [0.9ex] 
 $\mathbb{E}\big[S_N[f(\vect{Y})]\big]=\mathbb{E}[f(\vect{Y})]$   
& $\mathbb{E}[\hat{I}_{\textrm{MC}}]=\const{P}_{\variabile{j}, \Delta \optangle}$ \\ [0.9ex] 
 $\textrm{Var}\big[S_N[f(\vect{Y})]\big] = \frac{\textrm{Var}[f(\vect{Y})]}{N} = \frac{\sigma^2}{N}$  & $\textrm{Var}[\hat{I}_{\textrm{MC}}] = \frac{\const{P}_{\variabile{j}, \Delta \optangle}(1-\const{P}_{\variabile{j}, \Delta \optangle})}{\nrays} = \sigma_{\variabile{j}}^2$ \\ [0.9ex] 
 \hline
 \end{tabular}
\label{tab:MCcomparison}
 \end{table}

The main difference between the two methods is that in MC integration we know the probability density distribution $\rho$ of the random variables $\vect{Y}_{\variabile{i}}$ which is the uniform distribution. Because of this, the expected value of the function $f$ can be approximated by (\ref{eq:approx_MC_QMC}). On the contrary, in MC ray tracing we consider random variables equally distributed with a uniform distribution $\rho$ at the source but, applying ray tracing to these variables, we do not know the target probability distribution anymore. Therefore the expected value of the \textit{target} MC intensity $\hat{I}_{\textrm{MC}}$ cannot be approximated by the sum in (\ref{eq:approx_MC_QMC}). Thus, the target is divided into intervals (for the one dimensional case) of the same length, i.e., bins, and the intensity is approximated by the sum of the characteristic function $\chi_{\variabile{j}}$ of the $\variabile{j}$-th interval as explained in Chapter \ref{chap:raytracing} (see Equation (\ref{eq:approssimazione_MCraytracing})). Because of this, the error convergence also depends on to the number of bins for MC ray tracing (see Equation (\ref{eq:final_MC_error})).










